{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "362f1fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import os\n",
    "import nltk\n",
    "import re, string\n",
    "import treetaggerwrapper\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d282b63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#before uploading files from the dataset, files which weight 0 bytes and duplicates were removed\n",
    "#read each file in the dataset and put it to a list\n",
    "all_texts = []\n",
    "names = os.listdir(\"/Users/valeriia/archive\")\n",
    "for name in names:\n",
    "    if name.endswith(\".txt\"):\n",
    "        text = open(\"/Users/valeriia/archive/\"+name).read()\n",
    "        all_texts.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d00a767a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for removing special characters before tokenization\n",
    "#since \"&\" and \"%\" stand for actual words which are pronounced, they are kept but transformed into words\n",
    "#in order not to lose newlines \"\\n\" during tokenization, each newline is turned into \"###\"\n",
    "def remove_special_characters(sentence):\n",
    "    sentence = sentence.strip()\n",
    "    characters = r\"[.|,|:|!|?|*|(|)|~|-|–|„|“|●|#|/|\\\"|\\'|»|«]\"\n",
    "    sentence = re.sub(characters, r\"\", sentence)\n",
    "    repeat = r\"[x+0-9]\"\n",
    "    sentence = re.sub(repeat, r\"\", sentence)\n",
    "    if \"&\" in sentence:\n",
    "        sentence = re.sub(\"&\", r\" and \", sentence)\n",
    "    if \"%\" in sentence:\n",
    "        sentence = re.sub(\"%\", r\" Prozent\", sentence)\n",
    "    if \"-\" in sentence:\n",
    "        sentence = re.sub(\"-\", r\" \", sentence)\n",
    "    if \"\\n\" in sentence:\n",
    "        sentence = re.sub(\"\\n\", r\" ### \", sentence) \n",
    "    return sentence\n",
    "\n",
    "all_texts_cleaned = [remove_special_characters(sentence) for sentence in all_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df6a10fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if a word starts with a lower or a capital letter may be crucial in German\n",
    "#every single word in the set is tagged with POS\n",
    "#if POS of a word is anything but noun, it is converted to lower case\n",
    "tagger = treetaggerwrapper.TreeTagger(TAGLANG='de', TAGDIR=\"/Users/valeriia\")\n",
    "all_texts_cleaned2 = []\n",
    "for text in all_texts_cleaned:\n",
    "    text_tokens = []\n",
    "    text_pos = tagger.tag_text(text)\n",
    "    text_pos2 = treetaggerwrapper.make_tags(text_pos)\n",
    "    for word in text_pos2:\n",
    "        try:\n",
    "            if word.pos == \"NN\" or word.pos == \"NE\":\n",
    "                text_tokens.append(word.word)\n",
    "            else:\n",
    "                text_tokens.append((word.word).lower())\n",
    "        except:\n",
    "            continue\n",
    "    text_cleaned = \" \".join(text_tokens)\n",
    "    all_texts_cleaned2.append(text_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b7721f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for putting newlines \"/n\" back so that they can be used for marking borders between lines and refrains\n",
    "all_texts_cleaned3 = []\n",
    "def spaces(text):\n",
    "    if \"###\" in text:\n",
    "        text = re.sub(\" ### \", r\"\\n\", text)\n",
    "        text = re.sub(\"### \", r\"\\n\", text)\n",
    "        text = re.sub(\"###\", r\"\\n\", text)\n",
    "    all_texts_cleaned3.append(text)\n",
    "    \n",
    "for text in all_texts_cleaned2:\n",
    "    spaces(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc852990",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save texts to a json file - each song is a separate string\n",
    "jsonString = json.dumps(all_texts_cleaned3, indent=4, ensure_ascii=False)\n",
    "jsonFile = open(\"all_texts_cleaned.json\", \"w\", encoding='utf-8')\n",
    "jsonFile.write(jsonString)\n",
    "jsonFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f76ffd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the set into refrains\n",
    "refrains = []\n",
    "for text in all_texts_cleaned3:\n",
    "    refrain = \"\"\n",
    "    lines = text.splitlines( )\n",
    "    for line in lines:\n",
    "        refrain += line+\"/n\"\n",
    "        if line == \"\" and refrain != \"/n\":\n",
    "            if refrain.startswith(\"/n\"):\n",
    "                refrain = refrain[2:]\n",
    "            refrains.append(refrain)\n",
    "            refrain = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "facf5635",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save texts to a json file - each refrain is a separate string\n",
    "jsonString = json.dumps(refrains, indent=4, ensure_ascii=False)\n",
    "jsonFile = open(\"refrains_cleaned.json\", \"w\", encoding='utf-8')\n",
    "jsonFile.write(jsonString)\n",
    "jsonFile.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
