{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f32cff2e",
   "metadata": {},
   "source": [
    "### Table of contests\n",
    "1. [Data cleaning](#Data_cleaning)\n",
    "2. [GPT-2 models](#GPT_2_models)\n",
    "3. [Challenges and reflections](#Challenges_and_reflections)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db9cc76",
   "metadata": {},
   "source": [
    "### <a id=\"Data_cleaning\">Data cleaning</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "362f1fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import os\n",
    "import nltk\n",
    "import re, string\n",
    "import treetaggerwrapper\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d282b63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#upload data\n",
    "all_texts = []\n",
    "names = os.listdir(\"/Users/valeriia/Desktop/archive\")\n",
    "for name in names:\n",
    "    if name.endswith(\".txt\"):\n",
    "        text = open(\"/Users/valeriia/Desktop/archive/\"+name).read()\n",
    "        all_texts.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d00a767a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove special characters before tokenization\n",
    "def remove_characters_before_tokenization(sentence):\n",
    "    sentence = sentence.strip()\n",
    "    characters = r\"[.|,|:|!|?|*|(|)|~|-|–|„|“|●|#|/|\\\"|»|«]\"\n",
    "    sentence = re.sub(characters, r\"\", sentence)\n",
    "    repeat = r\"[x+0-9]\"\n",
    "    sentence = re.sub(repeat, r\"\", sentence)\n",
    "    if \"&\" in sentence:\n",
    "        sentence = re.sub(\"&\", r\" and \", sentence)\n",
    "    if \"%\" in sentence:\n",
    "        sentence = re.sub(\"%\", r\" Prozent\", sentence)\n",
    "    if \"-\" in sentence:\n",
    "        sentence = re.sub(\"-\", r\" \", sentence)\n",
    "    if \"'\" in sentence:\n",
    "        sentence = re.sub(\"'\", r\"\", sentence)\n",
    "    if \"\\n\" in sentence:\n",
    "        sentence = re.sub(\"\\n\", r\" ### \", sentence) #mark spaces \n",
    "    return sentence\n",
    "\n",
    "all_texts_cleaned = [remove_characters_before_tokenization(sentence) for sentence in all_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df6a10fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert all words except nouns to lower case\n",
    "tagger = treetaggerwrapper.TreeTagger(TAGLANG='de', TAGDIR=\"/Users/valeriia\")\n",
    "all_texts_cleaned2 = []\n",
    "for text in all_texts_cleaned:\n",
    "    text_tokens = []\n",
    "    text_pos = tagger.tag_text(text)\n",
    "    text_pos2 = treetaggerwrapper.make_tags(text_pos)\n",
    "    for word in text_pos2:\n",
    "        try:\n",
    "            if word.pos == \"NN\" or word.pos == \"NE\":\n",
    "                text_tokens.append(word.word)\n",
    "            else:\n",
    "                text_tokens.append((word.word).lower())\n",
    "        except:\n",
    "            continue\n",
    "    text_cleaned = \" \".join(text_tokens)\n",
    "    all_texts_cleaned2.append(text_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7721f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#put spaces \"/n\" back so that they can be used for marking borders between lines and refrains\n",
    "all_texts_cleaned3 = []\n",
    "def spaces(text):\n",
    "    if \"###\" in text:\n",
    "        text = re.sub(\" ### \", r\"\\n\", text)\n",
    "        text = re.sub(\"### \", r\"\\n\", text)\n",
    "        text = re.sub(\"###\", r\"\\n\", text)\n",
    "    all_texts_cleaned3.append(text)\n",
    "    \n",
    "for text in all_texts_cleaned2:\n",
    "    spaces(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc852990",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save texts to a json file - each song is a separate string\n",
    "jsonString = json.dumps(all_texts_cleaned3, indent=4, ensure_ascii=False)\n",
    "jsonFile = open(\"all_texts_cleaned.json\", \"w\", encoding='utf-8')\n",
    "jsonFile.write(jsonString)\n",
    "jsonFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f76ffd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the set into refrains\n",
    "refrains = []\n",
    "for text in all_texts_cleaned3:\n",
    "    refrain = \"\"\n",
    "    lines = text.splitlines( )\n",
    "    for line in lines:\n",
    "        refrain += line+\"/n\"\n",
    "        if line == \"\" and refrain != \"/n\":\n",
    "            if refrain.startswith(\"/n\"):\n",
    "                refrain = refrain[2:]\n",
    "            refrains.append(refrain)\n",
    "            refrain = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "facf5635",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save texts to a json file - each refrain is a separate string\n",
    "jsonString = json.dumps(refrains, indent=4, ensure_ascii=False)\n",
    "jsonFile = open(\"refrains.json\", \"w\", encoding='utf-8')\n",
    "jsonFile.write(jsonString)\n",
    "jsonFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf7617f",
   "metadata": {},
   "source": [
    "### <a id=\"GPT_2_models\">GPT-2 models</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7359b98",
   "metadata": {},
   "source": [
    "<a href=\"https://huggingface.co/dbmdz/german-gpt2\" target=\"_blank\">German GPT-2 model</a> was finetuned with <a href=\"https://www.kaggle.com/efrodl/german-rap-dataset\" target=\"_blank\">this rap dataset</a>. <a href=\"https://www.philschmid.de/fine-tune-a-non-english-gpt-2-model-with-huggingface\" target=\"_blank\">The following tutorial</a> was used as a basis for finetuning. For one model, the set was divided into separate songs; for another model - into separate refrains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "82915e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6329031",
   "metadata": {},
   "source": [
    "#### Examples for the first model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92f8aef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "lyrics1 = pipeline(\"text-generation\",model=\"gpt2_rap_lyrics_song_level\",tokenizer=\"anonymous-german-nlp/german-gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "946d26ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "heute geht die Meuterei los mit jedem Track wir sind zu weit unterwegs aber doch wir treffen uns immer mit keiner kann hier im Internet weiterreden denn ihr seid alle zu weit gekommen so viel habt ihr uns doch nicht geschenkt doch das alles ist die Geschichte\n"
     ]
    }
   ],
   "source": [
    "result = lyrics1('heute')[0]['generated_text']\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "64c79df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dieser Beat ist mein Ding und deshalb machen sich noch mehr Fans die hier nichts zu sagen haben keine Mühe sondern rappen an die Grenzen der Zeit wenn mir mal jemand ne Waffe zeigt doch wer hat schon geahnt dass das anders war wie man die Dinge zu\n"
     ]
    }
   ],
   "source": [
    "result = lyrics1('dieser Beat')[0]['generated_text']\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "984e594e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hassliebe ist nicht immer gut wenn ich die Augen zur Wand schließe Ich will nicht kratzen Ich hab’ keinen Bock drauf zu kratzen Ich sag’’ ich sag’ Ich hab’ kein Bock darauf zu kratzen Ich hab’ keinen Bock drauf\n"
     ]
    }
   ],
   "source": [
    "result = lyrics1('Hassliebe ist')[0]['generated_text']\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b970e186",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lass uns ab jetzt wir sind am rumfahr n und wir mach keinen Schade wenn s dann passiert wo is er so wie ein Hustler in der Schweiz wo ist der Typ der uns im Ghetto antreibt und was ist es in deinen Augen so\n"
     ]
    }
   ],
   "source": [
    "result = lyrics1('lass uns')[0]['generated_text']\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723117f4",
   "metadata": {},
   "source": [
    "#### Examples for the second model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1617dd64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "lyrics2 = pipeline(\"text-generation\",model=\"gpt2_rap_lyrics_refrain_level\",tokenizer=\"anonymous-german-nlp/german-gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "80d0d76e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "heute/n/n\",  \"Glaube ich will dich nicht mehr sehen/nIch hör die Stimme wie die Musik die mein Geist hörte/n/n\",  \"ihr kommt ja ich weiß was zu tun ist/nwas hat\n"
     ]
    }
   ],
   "source": [
    "result = lyrics2('heute')[0]['generated_text']\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0af5144a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dieser Beat/nSido und der Junge aus dem Haus der Nasty Banger/n/n\",  \"denn sie haben schon im Schrank geschlafen dann sind es noch zwei Stunden bis die Erde schäumt/nIch fliege zu meiner\n"
     ]
    }
   ],
   "source": [
    "result = lyrics2('dieser Beat')[0]['generated_text']\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1ea767ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hassliebe ist die Lüge die Lüge/nbist alle Menschen das ist Hass ist die Lüge/nder eine ist der der sich niemals fürchtet/ndie den Hass im Arm festhält/nAlles was ihn trifft wenn er auf Toilette wartet ist Hass\n"
     ]
    }
   ],
   "source": [
    "result = lyrics2('Hassliebe ist')[0]['generated_text']\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "50928c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lass uns noch zwei Wochen ins Hotel/nIch bleibe in meiner Welt mein Leben/ndann bleib ich dein Leben/nIch hab mein Geld in meine Hände gebracht/nIch war stolz auf Dich/nund ich habe den ganzen Tag gezaubert\n"
     ]
    }
   ],
   "source": [
    "result = lyrics2('lass uns')[0]['generated_text']\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4c721d",
   "metadata": {},
   "source": [
    "### <a id=\"Challenges_and_reflections\">Challenges and reflections</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d29fa1",
   "metadata": {},
   "source": [
    "<li>Out of two models, the second one which is trained on the refrain-level generates text which is subjectively closer to actual rap lyrics. Still, generated lyrics lack the right verse-form which is probably connected to the formatting of the data for finetuning.</li>\n",
    "<li>Since there is no strict verse structure, it's not possible to measure the rhyme.</li>\n",
    "<li>The dataset for training consisted of approx. 600 songs (50000 lines of lyrics). Ideally, the dataset should have been extended through additional lyrics which could be gathered by a lyrics scraper.</li>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
